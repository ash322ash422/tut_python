{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c90edf-e106-4838-881f-d988533574da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a46149-bc9f-4c88-bb73-5fefd17f1e31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03bce64c-168d-49eb-a1b5-a67de11fa0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we define a function that performs web scraping:\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "def scrape_books():\n",
    "    all_books = []\n",
    "\n",
    "    for page in range(1, 6):\n",
    "\n",
    "        url = f\"http://books.toscrape.com/catalogue/page-{page}.html\"\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        items = soup.find_all(\"article\", class_=\"product_pod\")\n",
    "\n",
    "        for item in items:\n",
    "            title = item.h3.a[\"title\"]\n",
    "            price = item.find(\"p\", class_=\"price_color\").text\n",
    "            availability = item.find(\n",
    "                \"p\", class_=\"instock availability\"\n",
    "            ).text.strip()\n",
    "\n",
    "            all_books.append([title, price, availability])\n",
    "\n",
    "    # Save using CSV module\n",
    "    with open(\"all_books.csv\", mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "        writer = csv.writer(file)\n",
    "\n",
    "        # Header\n",
    "        writer.writerow([\"Title\", \"Price\", \"Availability\"])\n",
    "\n",
    "        # Data rows\n",
    "        writer.writerows(all_books)\n",
    "\n",
    "    print(\"Data saved successfully!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40bf2b58-5233-4cd8-bcaa-746d0294f5a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved successfully!\n"
     ]
    }
   ],
   "source": [
    "scrape_books()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1490ba5d-af58-4e24-bfa4-c056b4097ba7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a39e2f0-38bb-437a-a216-53da01b8f352",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "595455ae-dacc-41bc-a01b-f8d66ba20dab",
   "metadata": {},
   "source": [
    "# FINAL python code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce04587c-deab-49f5-a0c1-1f90fd54776a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we define a function that performs web scraping:\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import schedule\n",
    "import time\n",
    "\n",
    "def scrape_books():\n",
    "    all_books = []\n",
    "\n",
    "    for page in range(1, 6):\n",
    "\n",
    "        url = f\"http://books.toscrape.com/catalogue/page-{page}.html\"\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        items = soup.find_all(\"article\", class_=\"product_pod\")\n",
    "\n",
    "        for item in items:\n",
    "            title = item.h3.a[\"title\"]\n",
    "            price = item.find(\"p\", class_=\"price_color\").text\n",
    "            availability = item.find(\n",
    "                \"p\", class_=\"instock availability\"\n",
    "            ).text.strip()\n",
    "\n",
    "            all_books.append([title, price, availability])\n",
    "\n",
    "    # Save using CSV module\n",
    "    with open(\"all_books.csv\", mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "        writer = csv.writer(file)\n",
    "\n",
    "        # Header\n",
    "        writer.writerow([\"Title\", \"Price\", \"Availability\"])\n",
    "\n",
    "        # Data rows\n",
    "        writer.writerows(all_books)\n",
    "\n",
    "    print(\"Data saved successfully!\")\n",
    "\n",
    "    \n",
    "\n",
    "# Scheduled Job\n",
    "def job():\n",
    "    print(\"Running scraper...\")\n",
    "    scrape_books()\n",
    "\n",
    "\n",
    "# Main Function\n",
    "def main():\n",
    "    # Schedule task\n",
    "    schedule.every().day.at(\"09:00\").do(job)\n",
    "\n",
    "    print(\"Scheduler started... Waiting for next run.\")\n",
    "\n",
    "    while True:\n",
    "        schedule.run_pending()\n",
    "        time.sleep(60)\n",
    "\n",
    "\n",
    "# Entry Point (BEST PRACTICE)\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
